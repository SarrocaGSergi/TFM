# Synthesising Images from text Descriptions

#### ABSTRACT
In the last years, image synthesis from text descriptions has become one of the most important research fields of computer vision. The aim of this field is to understand spatial relations between described objects and their position in the image, composing realistic images from these relationships. In this project, state-of-the-art text-to-image synthesis methods are studied and evaluated using the FashionGen dataset in order to study their behaviour, creating a fashion generator application as a result. We propose a text-to-image synthesis method to automatically generate clothing fashion images from natural language descriptions. The image generation is based on a Generative Adversarial Network trained using the FashionGen dataset, while the input is based on natural language descriptions processed by a text encoder with an attention mechanism and the data fusion is performed by the use of Conditional Batch Normalization in order to condition the image generation by text features. We purpose a new methodology as a benchmark for text-to-image synthesis by testing our method into three different GAN architectures and making it accessible and extendable to the rest of GANs by just a small change in their architectures.


> Google Slides: https://docs.google.com/presentation/d/1nCwbefOx-rrPQlFhCvmk8oVeEe0K-IX3fToiJn9An2c/edit?usp=sharing
